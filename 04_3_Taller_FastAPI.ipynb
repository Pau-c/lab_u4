{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99c51ec8",
      "metadata": {
        "id": "99c51ec8"
      },
      "source": [
        "\n",
        "# Taller (3h) — FastAPI + Scikit-Learn orientado a investigación\n",
        "**Objetivo:** Diseñar, entrenar y servir un modelo de ML **investigando** las decisiones técnicas clave.  \n",
        "**Entrega:** API funcional (FastAPI) + breve informe (markdown dentro del notebook) justificando decisiones.\n",
        "\n",
        "> Filosofía del taller: menos receta, más criterio. No hay una única respuesta correcta; lo evaluado es la **calidad del razonamiento**, la **limpieza de la implementación** y la **capacidad de probar** la API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdea1218",
      "metadata": {
        "id": "bdea1218"
      },
      "source": [
        "\n",
        "## Agenda sugerida (3h)\n",
        "1) **Planteo del problema y dataset** (30–40 min)  \n",
        "2) **Entrenamiento y persistencia** (40–50 min)  \n",
        "3) **Diseño del contrato y API** (45–55 min)  \n",
        "4) **Pruebas, errores y mejoras** (30–35 min)\n",
        "\n",
        "### Reglas\n",
        "- No borres los encabezados `TODO`. Agrega tu código debajo de cada bloque indicado.\n",
        "- Documenta tus decisiones en la sección **Bitácora** al final.\n",
        "- Puedes trabajar en equipo, pero cada entrega debe ser individual y original.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0cc07b6",
      "metadata": {
        "id": "e0cc07b6"
      },
      "source": [
        "\n",
        "---\n",
        "## 1) Selección de dataset y formulación del problema (investigación)\n",
        "Elige **uno**:\n",
        "- A) `sklearn.datasets.load_wine` (clasificación 3 clases, baseline rápido)\n",
        "- B) Un dataset tabular de UCI, Kaggle u otra fuente **citable** (debe ser pequeño/mediano y con licencia apta)\n",
        "- C) Un CSV propio (explica origen y variables)\n",
        "\n",
        "**Requisitos mínimos:**\n",
        "- Problema de **clasificación** o **regresión** tabular\n",
        "- Al menos **6 features numéricas** (puedes convertir categóricas)\n",
        "- Justifica por qué es un buen caso para servir vía API\n",
        "\n",
        "**Entrega (en esta celda, texto breve):** Describe el dataset, objetivo, variables y métrica principal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a51c33",
      "metadata": {
        "id": "15a51c33"
      },
      "source": [
        "\n",
        "### 1.1 Carga y exploración (TODO)\n",
        "- Carga el dataset (pd.read_csv o loader de sklearn).\n",
        "- Muestra `head()`, `describe()` y verifica nulos/outliers.\n",
        "- Selecciona `X` (features) y `y` (target); explica tu elección.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b92213bd",
      "metadata": {
        "id": "b92213bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Primeras 5 filas ---\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "--- DESCRIBE  ---\n",
            "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
            "count         150.000000        150.000000         150.000000   \n",
            "mean            5.843333          3.057333           3.758000   \n",
            "std             0.828066          0.435866           1.765298   \n",
            "min             4.300000          2.000000           1.000000   \n",
            "25%             5.100000          2.800000           1.600000   \n",
            "50%             5.800000          3.000000           4.350000   \n",
            "75%             6.400000          3.300000           5.100000   \n",
            "max             7.900000          4.400000           6.900000   \n",
            "\n",
            "       petal width (cm)      target  \n",
            "count        150.000000  150.000000  \n",
            "mean           1.199333    1.000000  \n",
            "std            0.762238    0.819232  \n",
            "min            0.100000    0.000000  \n",
            "25%            0.300000    0.000000  \n",
            "50%            1.300000    1.000000  \n",
            "75%            1.800000    2.000000  \n",
            "max            2.500000    2.000000  \n",
            "\n",
            "--- NULOS  ---\n",
            "sepal length (cm)    0\n",
            "sepal width (cm)     0\n",
            "petal length (cm)    0\n",
            "petal width (cm)     0\n",
            "target               0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Carga y EDA mínima\n",
        "import pandas as pd\n",
        "\n",
        "# from sklearn.datasets import load_iris\n",
        "\n",
        "# Ejemplo de estructura (reemplaza con tu fuente):\n",
        "# df = pd.read_csv(\"TU_DATASET.csv\")\n",
        "# ---- TU CÓDIGO AQUÍ ----\n",
        "\n",
        "# ---- DATASET DE PRUEBA- MODIFICAR ----\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris(as_frame=True)\n",
        "df = iris_data.frame\n",
        "\n",
        "#  Exploración\n",
        "print(\"--- Primeras 5 filas ---\")\n",
        "print(df.head())\n",
        "print(\"\\n--- DESCRIBE  ---\")\n",
        "print(df.describe())\n",
        "print(\"\\n--- NULOS  ---\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575da8f8",
      "metadata": {
        "id": "575da8f8"
      },
      "source": [
        "\n",
        "---\n",
        "## 2) Entrenamiento y persistencia del modelo (investigación)\n",
        "Toma decisiones y **justifícalas**:\n",
        "- ¿Modelo base? (p. ej. `LogisticRegression`, `RandomForest`, `XGBoost` si lo instalas)\n",
        "- ¿Preprocesamiento? (escala, imputación, OneHot, etc.)\n",
        "- ¿Validación? (`train_test_split` vs `cross_val_score`)\n",
        "- ¿Métrica? (clasificación: accuracy/F1; regresión: RMSE/MAE…)\n",
        "\n",
        "**Requisito:** empaqueta tu flujo en un `Pipeline` de sklearn y **persiste** el modelo y columnas (joblib + JSON).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6172f286",
      "metadata": {
        "id": "6172f286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Métricas de prueba: Accuracy = 0.9333\n"
          ]
        }
      ],
      "source": [
        "# TODO: split, pipeline, training, evaluación y persistencia\n",
        "from pathlib import Path\n",
        "import json, joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Variables esperadas:\n",
        "# X, y = ...\n",
        "# pipeline = ...\n",
        "# métricas calculadas en 'metrics' (dict)\n",
        "\n",
        "# ---- TU CÓDIGO AQUÍ ----\n",
        "\n",
        "# ---- DATASET DE PRUEBA- MODIFICAR ----\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 3. Selección de X (features) y y (target)\n",
        "X = df.drop(\n",
        "    columns=[\"target\"]\n",
        ")  # quedan todas las columnas excepto la que se va a predecir.\n",
        "X.columns = [\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"]\n",
        "# y (Target): La columna 'target' a predecir.\n",
        "y = df[\"target\"]\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# pipeline\n",
        "pipeline = Pipeline(\n",
        "    [(\"scaler\", StandardScaler()), (\"model\", KNeighborsClassifier(n_neighbors=5))]\n",
        ")\n",
        "\n",
        "# training\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# eval\n",
        "y_pred = pipeline.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# ---- DATASET DE PRUEBA- MODIFICAR END----\n",
        "\n",
        "#  diccionario de métricas\n",
        "metrics = {\n",
        "    \"test_accuracy\": round(test_accuracy, 4),\n",
        "    \"model_type\": \"KNeighborsClassifier\",\n",
        "    \"preprocessing\": \"StandardScaler\",\n",
        "}\n",
        "print(f\"\\nMétricas de prueba: Accuracy = {metrics['test_accuracy']}\")\n",
        "\n",
        "\n",
        "# TODO: Persistir\n",
        "# persiste modelo\n",
        "joblib.dump(pipeline, ARTIFACTS_DIR / \"model.joblib\")\n",
        "# persiste orden de cols\n",
        "json.dump(\n",
        "    {\"feature_columns\": list(X.columns)},\n",
        "    open(ARTIFACTS_DIR / \"feature_columns.json\", \"w\"),\n",
        ")\n",
        "# persiste metadatos y metricas\n",
        "json.dump({\"metrics\": metrics}, open(ARTIFACTS_DIR / \"metadata.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c134dd",
      "metadata": {
        "id": "00c134dd"
      },
      "source": [
        "\n",
        "---\n",
        "## 3) Diseño del contrato de la API (investigación)\n",
        "Define **endpoints mínimos**:\n",
        "- `GET /health` (estado, versión del modelo, métrica)\n",
        "- `POST /predict` (un registro)\n",
        "- `POST /predict-batch` (lista de registros)\n",
        "\n",
        "**Decisiones a justificar:**\n",
        "- ¿Qué validaciones aplicas en `Pydantic`? (rango, tipos, campos extra)\n",
        "- ¿Cómo garantizas el **orden** de columnas?\n",
        "- ¿Qué devuelves además de la predicción? (probabilidades, latencia, advertencias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "caf840dc",
      "metadata": {
        "id": "caf840dc"
      },
      "outputs": [],
      "source": [
        "# TODO: Generar un esquema dinámico a partir de las columnas persistidas (opcional pero recomendado)\n",
        "# Carga feature_columns.json y crea un dict para generar ejemplos de payload\n",
        "\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "feat_path = ARTIFACTS_DIR / \"feature_columns.json\"\n",
        "\n",
        "# ---- TU CÓDIGO AQUÍ ----\n",
        "\n",
        "#  Cargar la lista de columnas ( contrato de entrada)\n",
        "if not feat_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"El archivo de columnas no existe en {feat_path}. Ejecuta la Sección 2 (entrenamiento) primero.\"\n",
        "    )\n",
        "\n",
        "with open(feat_path, \"r\") as f:\n",
        "    column_data = json.load(f)\n",
        "\n",
        "#  lista de features siguiendo orden de entrenamiento\n",
        "feature_columns = column_data.get(\"feature_columns\", [])\n",
        "\n",
        "# Construye payload de ejemplo\n",
        "sample_payload = {}\n",
        "\n",
        "# Valores típicos de medidas del Iris están entre 4.0 y 8.0 cm\n",
        "EXAMPLE_VALUE = 5.5\n",
        "for col in feature_columns:\n",
        "    sample_payload[col] = EXAMPLE_VALUE\n",
        "\n",
        "# ---- FIN TU CÓDIGO AQUÍ ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8abce9f",
      "metadata": {
        "id": "f8abce9f"
      },
      "source": [
        "\n",
        "---\n",
        "## 4) Implementación de FastAPI (investigación)\n",
        "Crea un archivo `app.py` con:\n",
        "- Carga perezosa de `model.joblib` y `feature_columns.json`\n",
        "- Esquemas Pydantic (v2)\n",
        "- Endpoints `/health`, `/predict`, `/predict-batch`\n",
        "- Manejo de errores con `HTTPException` y mensajes claros\n",
        "\n",
        "**Pistas** (no copiar/pegar sin entender):\n",
        "- `model = joblib.load(...)`\n",
        "- `class Sample(BaseModel): ...`\n",
        "- `model.predict` y/o `model.predict_proba`\n",
        "- Retornar JSON con `dict | BaseModel`\n",
        "\n",
        "**Requisito:** esta celda **debe** escribir `app.py` con al menos la estructura básica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c41fccc6",
      "metadata": {
        "id": "c41fccc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'app.py escrito (plantilla con TODOs)'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Escribir app.py\n",
        "from textwrap import dedent\n",
        "\n",
        "app_code = dedent(\n",
        "    \"\"\"\n",
        "from logger_setup import setup_observability\n",
        "from logger_setup import check_input_ranges\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, ConfigDict, Field, field_validator\n",
        "from typing import List, Dict\n",
        "import joblib, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "\n",
        "APP_VERSION = \"0.1.0\"\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "MODEL_PATH = ARTIFACTS_DIR / \"model.joblib\"\n",
        "COLUMNS_PATH = ARTIFACTS_DIR / \"feature_columns.json\"\n",
        "META_PATH = ARTIFACTS_DIR / \"metadata.json\"\n",
        "\n",
        "app = FastAPI(title=\"ML API\", version=APP_VERSION)\n",
        "\n",
        "setup_observability(app)\n",
        "# Variables globales que se cargarán al inicio\n",
        "model = None\n",
        "columns = []\n",
        "meta = {}\n",
        "label_map = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"} #traduce la etiqueta en predicciones\n",
        "\n",
        "\n",
        "# TODO: Define tus modelos Pydantic aquí\n",
        "\n",
        "class Sample(BaseModel):\n",
        "    model_config = ConfigDict(extra=\"forbid\")\n",
        "    # ---- COMPLETA CON TUS CAMPOS SEGÚN feature_columns.json ----\n",
        "    \n",
        "    sepal_length_cm: float = Field(..., ge=3.0, le=8.5, description=\"Longitud del sépalo en cm\")\n",
        "    sepal_width_cm: float = Field(..., ge=1.5, le=5.0, description=\"Ancho del sépalo en cm\")\n",
        "    petal_length_cm: float = Field(..., ge=0.5, le=7.5, description=\"Longitud del pétalo en cm\")\n",
        "    petal_width_cm: float = Field(..., ge=0.0, le=3.0, description=\"Ancho del pétalo en cm\")\n",
        "\n",
        "    @field_validator(\"*\")\n",
        "    def no_nan(cls, v, field):\n",
        "        if v is None or math.isnan(v) or math.isinf(v):\n",
        "            raise ValueError(f\"Valor inválido en campo {field.name}\")\n",
        "        return v\n",
        "        \n",
        "class PredictionOut(BaseModel):\n",
        "    # ---- COMPLETA: qué devuelves? label / score / probs / latencia ----\n",
        "    \n",
        "    label_id: int = Field(..., description=\"ID de la categoria predicha\")\n",
        "    label_name: str = Field(..., description=\"Nombre de la categoria predicha.\")\n",
        "    score: float = Field(..., ge=0.0, le=1.0, description=\"confianza de la predicción.\")\n",
        "    latency_ms: float = Field(..., gr=0, description=\"Tiempo de procesamiento de la predicción en milisegundos.\")\n",
        "    pass\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_artifacts():\n",
        "    global model, columns, meta\n",
        "    if not MODEL_PATH.exists():\n",
        "        raise RuntimeError(\"Modelo no encontrado. Entrena y exporta primero.\")\n",
        "    model = joblib.load(MODEL_PATH)\n",
        "    columns = json.loads(COLUMNS_PATH.read_text())[\"feature_columns\"]\n",
        "    meta = json.loads(META_PATH.read_text()) if META_PATH.exists() else {}\n",
        "\n",
        "\n",
        "def prepare_data_for_model(data_samples: List[Sample], columns: List[str]) -> pd.DataFrame:\n",
        "    data_dicts = [sample.model_dump() for sample in data_samples]\n",
        "    X_input = pd.DataFrame(data_dicts, columns=columns)\n",
        "    return X_input\n",
        "\n",
        "#ENDPOINTS \n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"version\": APP_VERSION,\n",
        "        \"metrics\": meta.get(\"metrics\"),\n",
        "        \"n_features\": len(columns),\n",
        "        \"model_pipeline_steps\": [name for name, _ in model.steps] if model else \"N/A\" #estructura interna del modelo\n",
        "        }\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(sample: Sample):\n",
        "    try:\n",
        "        start = time.perf_counter()\n",
        "        # TODO: transforma 'sample' a lista en el orden de 'columns'\n",
        "        # y haz model.predict / predict_proba\n",
        "        # ---- TU CÓDIGO AQUÍ ----\n",
        "        \n",
        "        check_input_ranges(sample)\n",
        "        X_input = prepare_data_for_model([sample], columns)\n",
        "        #   predicciones\n",
        "        prediction_id = model.predict(X_input)[0]\n",
        "        prediction_proba = model.predict_proba(X_input)[0]\n",
        "        \n",
        "        # Calcular la confianza \n",
        "        score = np.max(prediction_proba)\n",
        "        \n",
        "        # latencia y formatear salida\n",
        "        latency_ms = (time.perf_counter() - start) * 1000\n",
        "        \n",
        "        # con el mapa se obtiene el nombre de la etiqueta\n",
        "        label_name = label_map.get(prediction_id, \"Desconocida\")\n",
        "        \n",
        "        return PredictionOut(\n",
        "            label_id=int(prediction_id),\n",
        "            label_name=label_name,\n",
        "            score=round(score, 4),\n",
        "            latency_ms=round(latency_ms, 3)\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error en /predict: {e}\")\n",
        "        raise HTTPException(status_code=400, detail=f\"Error en el procesamiento de la predicción: {str(e)}\")\n",
        "\n",
        "\n",
        "@app.post(\"/predict-batch\")\n",
        "def predict_batch(samples: List[Sample]):\n",
        "    try:\n",
        "        # TODO: similar a /predict pero para lista\n",
        "        # ---- TU CÓDIGO AQUÍ ----\n",
        "        \n",
        "        for sample in samples:\n",
        "            check_input_ranges(sample)\n",
        "        start = time.perf_counter()\n",
        "        X_input = prepare_data_for_model(samples, columns)\n",
        "        \n",
        "        #  predicciones\n",
        "        prediction_ids = model.predict(X_input)\n",
        "        prediction_probas = model.predict_proba(X_input)\n",
        "        \n",
        "        results = []\n",
        "        for id_pred, probas in zip(prediction_ids, prediction_probas):\n",
        "            score = np.max(probas)\n",
        "            label_name = label_map.get(id_pred, \"Desconocida\")\n",
        "            \n",
        "            results.append(PredictionOut(\n",
        "                label_id=int(id_pred),\n",
        "                label_name=label_name,\n",
        "                score=round(score, 4),\n",
        "                latency_ms=0.0 \n",
        "            ))\n",
        "\n",
        "        # Reemplaza latency_ms individual con el tiempo total del batch\n",
        "        total_latency_ms = (time.perf_counter() - start) * 1000\n",
        "        \n",
        "        # Para el batch, se actualiza el primer elemento \n",
        "        if results:\n",
        "            results[0].latency_ms = round(total_latency_ms, 3)\n",
        "            \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error en /predict-batch: {e}\")\n",
        "        raise HTTPException(status_code=400, detail=f\"Error en el procesamiento del lote: {str(e)}\")\n",
        "\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "Path(\"app.py\").write_text(app_code, encoding=\"utf-8\")\n",
        "\"app.py escrito (plantilla con TODOs)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3effd9b0",
      "metadata": {
        "id": "3effd9b0"
      },
      "source": [
        "\n",
        "### 4.1 Ejecutar el servidor\n",
        "```bash\n",
        "uvicorn app:app --reload --port 8000\n",
        "```\n",
        "Abre `http://127.0.0.1:8000/docs` y prueba manualmente. Registra resultados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59771f1d",
      "metadata": {
        "id": "59771f1d"
      },
      "source": [
        "\n",
        "---\n",
        "## 5) Pruebas y casos límite (investigación)\n",
        "Define y ejecuta **al menos 6** pruebas:\n",
        "- 3 válidas (predict y batch)\n",
        "- 3 inválidas (campo faltante, tipo incorrecto, campo extra, etc.)\n",
        "\n",
        "Incluye código de prueba y captura de respuestas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cbc5c4f6",
      "metadata": {
        "id": "cbc5c4f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Ejecutando pruebas contra la API en http://127.0.0.1:8000\n",
            "\n",
            "--- PRUEBA: 0. GET /health (Verificación de Carga de Modelo) ---\n",
            "RESULTADO: PASSED. El modelo y los metadatos se cargaron correctamente.\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- PRUEBA: 2. POST /predict (Válido - Clase 0) ---\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "{\n",
            "    \"label_id\": 0,\n",
            "    \"label_name\": \"Setosa\",\n",
            "    \"score\": 1.0,\n",
            "    \"latency_ms\": 7.771\n",
            "}\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 3. POST /predict (Válido - Clase 2) ---\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "{\n",
            "    \"label_id\": 2,\n",
            "    \"label_name\": \"Virginica\",\n",
            "    \"score\": 1.0,\n",
            "    \"latency_ms\": 6.907\n",
            "}\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 4. POST /predict-batch (Válido) ---\n",
            "Status Code: 200\n",
            "Response Body:\n",
            "[\n",
            "    {\n",
            "        \"label_id\": 0,\n",
            "        \"label_name\": \"Setosa\",\n",
            "        \"score\": 1.0,\n",
            "        \"latency_ms\": 4.767\n",
            "    },\n",
            "    {\n",
            "        \"label_id\": 1,\n",
            "        \"label_name\": \"Versicolor\",\n",
            "        \"score\": 0.8,\n",
            "        \"latency_ms\": 0.0\n",
            "    },\n",
            "    {\n",
            "        \"label_id\": 2,\n",
            "        \"label_name\": \"Virginica\",\n",
            "        \"score\": 1.0,\n",
            "        \"latency_ms\": 0.0\n",
            "    }\n",
            "]\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 5. POST /predict (Inválido - Falta 'sepal_length_cm') ---\n",
            "Status Code: 422\n",
            "Response Body:\n",
            "{\n",
            "    \"detail\": [\n",
            "        {\n",
            "            \"type\": \"missing\",\n",
            "            \"loc\": [\n",
            "                \"body\",\n",
            "                \"sepal_length_cm\"\n",
            "            ],\n",
            "            \"msg\": \"Field required\",\n",
            "            \"input\": {\n",
            "                \"sepal_width_cm\": 3.5,\n",
            "                \"petal_length_cm\": 1.4,\n",
            "                \"petal_width_cm\": 0.2\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 6. POST /predict (Inválido - Tipo de dato incorrecto en 'sepal_width_cm') ---\n",
            "Status Code: 422\n",
            "Response Body:\n",
            "{\n",
            "    \"detail\": [\n",
            "        {\n",
            "            \"type\": \"float_parsing\",\n",
            "            \"loc\": [\n",
            "                \"body\",\n",
            "                \"sepal_width_cm\"\n",
            "            ],\n",
            "            \"msg\": \"Input should be a valid number, unable to parse string as a number\",\n",
            "            \"input\": \"valor_incorrecto\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 7. POST /predict (Inválido - Campo Extra no permitido) ---\n",
            "Status Code: 422\n",
            "Response Body:\n",
            "{\n",
            "    \"detail\": [\n",
            "        {\n",
            "            \"type\": \"extra_forbidden\",\n",
            "            \"loc\": [\n",
            "                \"body\",\n",
            "                \"campo_extra_no_permitido\"\n",
            "            ],\n",
            "            \"msg\": \"Extra inputs are not permitted\",\n",
            "            \"input\": 123.0\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "----------------------------------------\n",
            "\n",
            "--- PRUEBA: 8. POST /predict (Latencia) ---\n",
            "Umbral: 500.00 ms\n",
            "Latencia medida: 37.198 ms\n",
            "Resultado: PASSED\n",
            "Status Code de la respuesta: 200\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# TODO: pruebas con requests (server debe estar corriendo)\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "# --- Configuración y Carga de Artefactos ---\n",
        "BASE_URL = \"http://127.0.0.1:8000\"\n",
        "ARTIFACTS_DIR = Path(\"artifacts\")\n",
        "COLUMNS_PATH = ARTIFACTS_DIR / \"feature_columns.json\"\n",
        "\n",
        "\n",
        "#  PRUEBA DE CARGA Y ESTADO  ---\n",
        "def test_model_load_status(base_url):\n",
        "    # verifica que el modelo se cargó correctamente y que el health check es exitoso\n",
        "    try:\n",
        "        r = requests.get(f\"{base_url}/health\", timeout=5)\n",
        "\n",
        "        print(\"\\n--- PRUEBA: 0. GET /health (Verificación de Carga de Modelo) ---\")\n",
        "\n",
        "        #  Verificar el código de estado (200)\n",
        "        assert (\n",
        "            r.status_code == 200\n",
        "        ), f\"FALLO: Status code no es 200. Recibido: {r.status_code}\"\n",
        "        data = r.json()\n",
        "\n",
        "        # que el modelo está en estado 'ok'\n",
        "        assert (\n",
        "            data.get(\"status\") == \"ok\"\n",
        "        ), f\"FALLO: Estado no es 'ok'. Recibido: {data.get('status')}\"\n",
        "\n",
        "        #  que la estructura del pipeline se cargó\n",
        "        model_steps = data.get(\"model_pipeline_steps\")\n",
        "        assert (\n",
        "            model_steps != \"N/A\"\n",
        "        ), \"FALLO: El pipeline del modelo no se cargó correctamente (es 'N/A').\"\n",
        "\n",
        "        #  que las métricas y las features se cargaron\n",
        "        assert data.get(\"metrics\") is not None, \"FALLO: Métricas no cargadas.\"\n",
        "        assert (\n",
        "            data.get(\"n_features\", 0) > 0\n",
        "        ), f\"FALLO: Número de features es 0. Recibido: {data.get('n_features')}\"\n",
        "\n",
        "        print(\"RESULTADO: PASSED. El modelo y los metadatos se cargaron correctamente.\")\n",
        "        print(\"-\" * 60)\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\n",
        "            \"\\n[ERROR FATAL] No se pudo conectar a la API. El servidor Uvicorn debe estar corriendo.\"\n",
        "        )\n",
        "        return False\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        print(f\"Response Body para depuración:\\n{json.dumps(data, indent=4)}\")\n",
        "        print(\"RESULTADO: FAILED. Error en la carga de artefactos.\")\n",
        "        print(\"-\" * 60)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error inesperado durante la verificación de carga: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "#  Cargar la lista de columnas\n",
        "try:\n",
        "    with open(COLUMNS_PATH, \"r\") as f:\n",
        "        column_data = json.load(f)\n",
        "    # Lista de nombres que espera pydantic\n",
        "    pydantic_columns = column_data.get(\"feature_columns\", [])\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\n",
        "        \"feature_columns.json no encontrado. Ejecuta la Sección 2 (entrenamiento).\"\n",
        "    )\n",
        "\n",
        "#  Cargar el dataset original\n",
        "iris_data = load_iris(as_frame=True)\n",
        "df_full = iris_data.frame\n",
        "\n",
        "# Obtener los nombres originales de las features\n",
        "original_columns = df_full.columns.drop(\"target\").tolist()\n",
        "\n",
        "# DataFrame con las features originales y el target\n",
        "df_test = df_full[original_columns + [\"target\"]].copy()\n",
        "\n",
        "# ---  PAYLOADS DE PRUEBA ---\n",
        "\n",
        "# Selecciona UNA fila por cada valor único de la columna 'target'\n",
        "sample_rows = df_test.groupby(\"target\", observed=True).first().reset_index()\n",
        "#  Convertir las filas de muestras a diccionarios\n",
        "payloads_validos = []\n",
        "\n",
        "for _, row in sample_rows.iterrows():\n",
        "\n",
        "    feature_values = row[original_columns].tolist()\n",
        "\n",
        "    payload = dict(zip(pydantic_columns, feature_values))\n",
        "    payloads_validos.append(payload)\n",
        "\n",
        "# PAYLOADS  FINALES\n",
        "payload_valido_1 = payloads_validos[0]  # Muestra de la primera clase\n",
        "payload_valido_2 = payloads_validos[1]  #  la segunda clase\n",
        "\n",
        "if len(payloads_validos) > 2:\n",
        "    payload_valido_3 = payloads_validos[2]  # tercera clase\n",
        "else:\n",
        "    # Caso para datasets con menos de 3 clases, se usa el payload 1 para la prueba extra\n",
        "    payload_valido_3 = payloads_validos[0]\n",
        "\n",
        "\n",
        "# --- PAYLOADS INVÁLIDOS ---\n",
        "\n",
        "#  Inválido: Falta un campo requerido (saca el primer campo Pydantic)\n",
        "col_faltante = pydantic_columns[0]\n",
        "payload_invalido_faltante = payload_valido_1.copy()\n",
        "del payload_invalido_faltante[col_faltante]\n",
        "\n",
        "\n",
        "#  Inválido: Tipo de dato incorrecto (envia string en vez de float)\n",
        "col_tipo_incorrecto = pydantic_columns[1]\n",
        "payload_invalido_tipo = payload_valido_2.copy()\n",
        "payload_invalido_tipo[col_tipo_incorrecto] = \"valor_incorrecto\"  # Envia string\n",
        "\n",
        "\n",
        "#  Inválido: Campo extra no permitido (extra='forbid' en Pydantic)\n",
        "payload_invalido_extra = payload_valido_3.copy()\n",
        "payload_invalido_extra[\"campo_extra_no_permitido\"] = 123.0\n",
        "\n",
        "\n",
        "# PAYLOAD BATCH VÁLIDO (lista de los 3 payloads válidos)\n",
        "payload_batch_valido = (\n",
        "    payloads_validos if len(payloads_validos) >= 3 else [payload_valido_1] * 3\n",
        ")\n",
        "\n",
        "\n",
        "# ---   EJECUCIÓN DE PRUEBAS ---\n",
        "\n",
        "\n",
        "def print_result(name, r):\n",
        "    # Imprime el resultado deL REQ\n",
        "    try:\n",
        "        data = r.json()\n",
        "    except json.JSONDecodeError:\n",
        "        data = r.text\n",
        "\n",
        "    print(f\"\\n--- PRUEBA: {name} ---\")\n",
        "    print(f\"Status Code: {r.status_code}\")\n",
        "    print(f\"Response Body:\\n{json.dumps(data, indent=4)}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "print(\"\\n[INFO] Ejecutando pruebas contra la API en\", BASE_URL)\n",
        "\n",
        "\n",
        "# latencia\n",
        "def test_latency(url, payload, threshold_ms):\n",
        "    # Mide la latencia de una solicitud POST y evalúa si está por debajo del umbral\n",
        "    start_time = time.perf_counter()\n",
        "    r = requests.post(url, json=payload, timeout=5)\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    latency_ms = (end_time - start_time) * 1000\n",
        "    passed = latency_ms < threshold_ms\n",
        "\n",
        "    status = \"PASSED\" if passed and r.status_code == 200 else \"FAILED\"\n",
        "\n",
        "    print(f\"\\n--- PRUEBA: 8. POST /predict (Latencia) ---\")\n",
        "    print(f\"Umbral: {threshold_ms:.2f} ms\")\n",
        "    print(f\"Latencia medida: {latency_ms:.3f} ms\")\n",
        "    print(f\"Resultado: {status}\")\n",
        "    print(f\"Status Code de la respuesta: {r.status_code}\")\n",
        "    print(\"-\" * 40)\n",
        "    return r\n",
        "\n",
        "\n",
        "LATENCY_THRESHOLD_MS = 500  # 0,5 segundos\n",
        "\n",
        "try:\n",
        "    # Prueba 1: GET /health\n",
        "    if test_model_load_status(BASE_URL):\n",
        "        pass\n",
        "\n",
        "    # --- PRUEBAS VÁLIDAS (Se espera 200 OK) ---\n",
        "\n",
        "    # Prueba 2: POST /predict (Caso Válido 1 - Setosa)\n",
        "    r_pred_valida_1 = requests.post(\n",
        "        f\"{BASE_URL}/predict\", json=payload_valido_1, timeout=5\n",
        "    )\n",
        "    print_result(f\"2. POST /predict (Válido - Clase 0)\", r_pred_valida_1)\n",
        "\n",
        "    # Prueba 3: POST /predict (Caso Válido 2 - Virginica o Clase 2)\n",
        "    r_pred_valida_2 = requests.post(\n",
        "        f\"{BASE_URL}/predict\", json=payload_valido_3, timeout=5\n",
        "    )\n",
        "    print_result(\n",
        "        f\"3. POST /predict (Válido - Clase {len(payloads_validos)-1})\", r_pred_valida_2\n",
        "    )\n",
        "\n",
        "    # Prueba 4: POST /predict-batch (Caso Válido con 3 registros)\n",
        "    r_batch_valido = requests.post(\n",
        "        f\"{BASE_URL}/predict-batch\", json=payload_batch_valido, timeout=10\n",
        "    )\n",
        "    print_result(\"4. POST /predict-batch (Válido)\", r_batch_valido)\n",
        "\n",
        "    # --- PRUEBAS INVÁLIDAS (Se espera 422 ) ---\n",
        "\n",
        "    # Prueba 5: POST /predict (Inválido - Falta campo)\n",
        "    r_inv_faltante = requests.post(\n",
        "        f\"{BASE_URL}/predict\", json=payload_invalido_faltante, timeout=5\n",
        "    )\n",
        "    print_result(\n",
        "        f\"5. POST /predict (Inválido - Falta '{col_faltante}')\", r_inv_faltante\n",
        "    )\n",
        "\n",
        "    # Prueba 6: POST /predict (Inválido - Tipo incorrecto)\n",
        "    r_inv_tipo = requests.post(\n",
        "        f\"{BASE_URL}/predict\", json=payload_invalido_tipo, timeout=5\n",
        "    )\n",
        "    print_result(\n",
        "        f\"6. POST /predict (Inválido - Tipo de dato incorrecto en '{col_tipo_incorrecto}')\",\n",
        "        r_inv_tipo,\n",
        "    )\n",
        "\n",
        "    # Prueba 7: POST /predict (Inválido - Campo extra)\n",
        "    r_inv_extra = requests.post(\n",
        "        f\"{BASE_URL}/predict\", json=payload_invalido_extra, timeout=5\n",
        "    )\n",
        "    print_result(\"7. POST /predict (Inválido - Campo Extra no permitido)\", r_inv_extra)\n",
        "\n",
        "    # Prueba 8: latencia ---\n",
        "    test_latency_result = test_latency(\n",
        "        f\"{BASE_URL}/predict\", payload_valido_1, LATENCY_THRESHOLD_MS\n",
        "    )\n",
        "\n",
        "    if not (\n",
        "        test_latency_result.status_code == 200\n",
        "        and test_latency_result.elapsed.total_seconds() * 1000 < LATENCY_THRESHOLD_MS\n",
        "    ):\n",
        "        print(\n",
        "            f\"[ADVERTENCIA] La prueba de latencia falló (código {test_latency_result.status_code}).\"\n",
        "        )\n",
        "\n",
        "\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\n",
        "        \"\\n[ERROR] No se pudo conectar a la API. El servidor Uvicorn tiene que estar corriendo en el puerto 8000.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "662bd072",
      "metadata": {
        "id": "662bd072"
      },
      "source": [
        "\n",
        "---\n",
        "## 6) (Opcional) Observabilidad y despliegue (investigación)\n",
        "- Middleware de latencia y logger estructurado (añade header `X-Process-Time-ms`)\n",
        "- Manejo de warnings cuando la entrada está fuera de rango esperado\n",
        "- Dockerfile mínimo para empaquetar y correr localmente\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bf0a67ab",
      "metadata": {
        "id": "bf0a67ab"
      },
      "outputs": [],
      "source": [
        "# TODO (opcional): pega aquí snippets de middleware o Dockerfile que diseñes\n",
        "# class TimingMiddleware(...): ...\n",
        "# FROM python:3.11-slim\n",
        "pass\n",
        "\n",
        "# VER ARCHIVO logger_setup.py PARA EL CODIGO DEL LOGGER, INSPECCIONAR API_LOGS/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d5fcca",
      "metadata": {
        "id": "60d5fcca"
      },
      "source": [
        "\n",
        "---\n",
        "## 7) Criterios de evaluación (rúbrica breve)\n",
        "- **Justificación técnica (25%)**: dataset, métrica, modelo y preprocesamiento argumentados.\n",
        "- **Calidad del pipeline (20%)**: reproducibilidad y limpieza (Pipeline, persistencia correcta).\n",
        "- **Contrato y validaciones (25%)**: Pydantic coherente, errores claros, orden de features garantizado.\n",
        "- **Pruebas (20%)**: variedad de casos, evidencia de resultados y manejo de fallos.\n",
        "- **Código y documentación (10%)**: legibilidad, estructura y claridad de mensajes.\n",
        "\n",
        "---\n",
        "## Bitácora de decisiones (responde aquí)\n",
        "- Dataset y objetivo:\n",
        "- Selección de features/target:\n",
        "- Modelo y preprocesamiento:\n",
        "- Métrica principal y resultados:\n",
        "- Decisiones de contrato (payload, validaciones, respuestas):\n",
        "- Observabilidad y pruebas:\n",
        "- Lecciones aprendidas:\n",
        "\n",
        "VER README para bitacora/justificaciones técnicas"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab_u4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
